{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8c92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-28 17:29:45 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json, os, random\n",
    "import pdb\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "import hydra \n",
    "from omegaconf import DictConfig, OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d1fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystrip(one_str):\n",
    "    one_str = one_str.strip()\n",
    "    one_str = one_str.strip(\"\\\\n\")\n",
    "    one_str = one_str.strip(\"#\")\n",
    "    return one_str\n",
    "\n",
    "def extract_substring2(text, start_str, stop_strs):\n",
    "    start_index = text.find(start_str)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    start = start_index + len(start_str)\n",
    "    \n",
    "    end = len(text)\n",
    "    \n",
    "    for stop_str in stop_strs:\n",
    "        temp_index = text.find(stop_str, start)\n",
    "        if temp_index != -1 and temp_index < end:\n",
    "            end = temp_index\n",
    "    if start < end:\n",
    "        return mystrip(text[start:end])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def split_response(response):\n",
    "    mydict = {\n",
    "        \"original\":response\n",
    "    }\n",
    "    str_analysis = \"The problem analysis:\"\n",
    "    str_query = \"The retrieval query:\"\n",
    "    str_answer = \"The final answer:\"\n",
    "    stop_strs = [str_analysis, str_query, str_answer, \"The retrieval documents:\", \"###\", \"####\"]\n",
    "    stop_strs_query = [str_analysis, str_query, str_answer, \"The retrieval documents:\", \"###\", \"####\", \"\\nStep\", \"?\"]\n",
    "    stop_strs_answer = [str_analysis, str_query, str_answer, \"The retrieval documents:\", \"###\", \"####\", \"\\nStep\"]\n",
    "    \n",
    "    start_index = response.find(str_analysis)\n",
    "    if start_index==-1:     \n",
    "        mydict['analysis']=None\n",
    "        return mydict\n",
    "    else:\n",
    "        mydict[\"analysis\"]=extract_substring2(response, str_analysis, stop_strs)\n",
    "\n",
    "    start_index_query = response.find(str_query, start_index+len(str_analysis))\n",
    "    start_index_answer = response.find(str_answer, start_index+len(str_analysis))\n",
    "\n",
    "    if start_index_query==-1 and start_index_answer==-1:\n",
    "        mydict['analysis']=None\n",
    "        return mydict\n",
    "    elif start_index_query!=-1 and start_index_answer!=-1:\n",
    "        if start_index_query<start_index_answer:\n",
    "            mydict['query']=extract_substring2(response[start_index_query:], str_query, stop_strs_query)\n",
    "        else:\n",
    "            mydict['answer']=extract_substring2(response[start_index_answer:], str_answer, stop_strs_answer)\n",
    "    elif start_index_query!=-1:\n",
    "        mydict['query']=extract_substring2(response[start_index_query:], str_query, stop_strs_query)\n",
    "    elif start_index_answer!=-1:\n",
    "        mydict['answer']=extract_substring2(response[start_index_answer:], str_answer, stop_strs_answer)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return mydict\n",
    "\n",
    "def solve(cfg: DictConfig):\n",
    "    ckpt , records = solve_init(cfg)\n",
    "    solve_main(cfg, ckpt, records)\n",
    "    \n",
    "    remain_idxs = [i for i , record in enumerate(records) if 'answer' not in record]\n",
    "    print(f\"Remain records: {len(remain_idxs)}\")\n",
    "\n",
    "    if len(remain_idxs) > 0:\n",
    "        solve_directly(cfg, ckpt, records)\n",
    "    \n",
    "    output_file = os.path.join(os.getcwd(), \"records.jsonl\")\n",
    "    print(f\"Saving records to {output_file}\")\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for record in records:\n",
    "            json.dump(record, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def solve_init(cfg: DictConfig):\n",
    "    if cfg.debug:\n",
    "        ckpt = LLM(\n",
    "            model=cfg.model.path, \n",
    "            tensor_parallel_size=1,\n",
    "        )\n",
    "    else:\n",
    "        ckpt = LLM(\n",
    "            model=cfg.model.path, \n",
    "            tensor_parallel_size=cfg.model.tensor_parallel_size\n",
    "        )\n",
    "    print(\"ckpt is ready.\")\n",
    "    \n",
    "    dataset = dataset = load_dataset('hotpotqa/hotpot_qa', 'fullwiki')['validation']\n",
    "\n",
    "    if cfg.debug:\n",
    "        dataset_size = len(dataset)\n",
    "        sample_size = min(8, dataset_size)\n",
    "        sampled_indices = random.sample(range(dataset_size), sample_size)\n",
    "        dataset = dataset.select(sampled_indices)\n",
    "\n",
    "    records = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        record = {\n",
    "            'question': data['question'],\n",
    "            'golden_answers': data['answer'],\n",
    "            'state': \"undo\",\n",
    "            'resample_times': 0\n",
    "        }\n",
    "        records.append(record)\n",
    "    return ckpt , records\n",
    "\n",
    "def generate_naive_generation_cot_prompt(question):\n",
    "    system_message = \"\"\"You are a helpful assistant that thinks through problems step by step before providing a final answer based on your own knowledge.\n",
    "\n",
    "For any question, please structure your response in this format:\n",
    "The problem analysis: [Provide detailed step-by-step reasoning]\n",
    "The final answer: [Provide the concise final answer]\n",
    "\n",
    "Example:\n",
    "User: What is 25 × 36?\n",
    "Assistant:\n",
    "The problem analysis: I need to multiply 25 by 36.\n",
    "I can break this down:\n",
    "25 × 36 = 25 × (30 + 6)\n",
    "= 25 × 30 + 25 × 6\n",
    "= 750 + 150\n",
    "= 900\n",
    "The final answer: 25 × 36 = 900\n",
    "\n",
    "Please think through each question carefully, breaking down complex problems into manageable steps.\"\"\"\n",
    "    user_message = f\"\"\"The question: {question}\"\"\"\n",
    "    message_list = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}]\n",
    "    return message_list\n",
    "\n",
    "def generate_naive_generation_prompt(question):\n",
    "    system_message = \"\"\" Answer the question based on your own knowledge. Only give me the answer and do not output any other words.\"\"\"\n",
    "    user_message = f\"\"\"The question: {question}\"\"\"\n",
    "    message_list = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}]\n",
    "    return message_list\n",
    "\n",
    "def solve_main(cfg: DictConfig, ckpt, records):\n",
    "    sampling_params = SamplingParams(temperature=cfg.params.temperature, max_tokens=cfg.params.max_tokens)\n",
    "    \n",
    "    messages = [generate_naive_generation_cot_prompt(record['question']) for record in records]\n",
    "    outputs = ckpt.chat(messages, sampling_params)\n",
    "    outputs = [output.outputs[0].text for output in outputs]\n",
    "    vals = [split_response(output) for output in outputs]\n",
    "        \n",
    "    for i, val in enumerate(vals):\n",
    "        records[i]['output'] = val['original']\n",
    "        if 'answer' in val and val['answer'] is not None:\n",
    "            records[i]['answer'] = val['answer']\n",
    "            records[i]['state'] = \"done\"\n",
    "        else:\n",
    "            records[i]['state'] = \"wrong\"\n",
    "\n",
    "def solve_directly(cfg: DictConfig, ckpt, records):\n",
    "    sampling_params = SamplingParams(temperature=cfg.params.temperature, max_tokens=cfg.params.max_tokens)\n",
    "    \n",
    "    remain_idxs = [i for i, record in enumerate(records) if 'answer' not in record]\n",
    "    messages = [generate_naive_generation_prompt(records[remain_idx]['question']) for remain_idx in remain_idxs]\n",
    "    \n",
    "    outputs = ckpt.chat(messages, sampling_params)\n",
    "    outputs = [output.outputs[0].text for output in outputs]\n",
    "        \n",
    "    for output, remain_idx in zip(outputs, remain_idxs):   \n",
    "        records[remain_idx]['answer'] = output\n",
    "        records[remain_idx]['state'] = \"done\"\n",
    "        records[remain_idx]['resample_times'] = records[remain_idx].get('resample', 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd986d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\n",
      "- _self_\n",
      "debug: false\n",
      "model:\n",
      "  path: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  tensor_parallel_size: 1\n",
      "data:\n",
      "  dev_dataset_path: /mnt/raid5/kangjh/downloads/datasets/hotpotqa/dev/dev.json\n",
      "params:\n",
      "  temperature: 0.0\n",
      "  max_tokens: 512\n",
      "\n",
      "Start at 2025-09-28 17:29:48\n",
      "INFO 09-28 17:29:48 [utils.py:328] non-default args: {'disable_log_stats': True, 'model': 'meta-llama/Meta-Llama-3-8B-Instruct'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-28 17:29:57 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-28 17:29:57 [__init__.py:1815] Using max model len 8192\n",
      "INFO 09-28 17:30:01 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:02 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:02 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:06 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m WARNING 09-28 17:30:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:06 [gpu_model_runner.py:2338] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W928 17:30:06.073281463 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:07 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:07 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:07 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:08 [weight_utils.py:369] Time spent downloading weights for meta-llama/Meta-Llama-3-8B-Instruct: 0.591155 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40171d7305be41859e1ff7c5b79446f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:11 [default_loader.py:268] Loading weights took 2.83 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:12 [gpu_model_runner.py:2392] Model loading took 14.9596 GiB and 4.429412 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:17 [backends.py:539] Using cache directory: /home/kangjh/.cache/vllm/torch_compile_cache/ec52f37144/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:17 [backends.py:550] Dynamo bytecode transform time: 5.32 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:19 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.827 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:21 [monitor.py:34] torch.compile takes 5.32 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:23 [gpu_worker.py:298] Available KV cache memory: 26.48 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:23 [kv_cache_utils.py:864] GPU KV cache size: 216,880 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:23 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 26.47x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:08<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:32 [gpu_model_runner.py:3118] Graph capturing finished in 9 secs, took 0.53 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:32 [gpu_worker.py:391] Free memory on device (47.13/47.43 GiB) on startup. Desired GPU memory utilization is (0.9, 42.69 GiB). Actual usage is 14.96 GiB for weight, 1.24 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.53 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=27701056921` to fit into requested memory, or `--kv-cache-memory=32465419264` to fully utilize gpu memory. Current kv cache memory in use is 28428768665 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1323835)\u001b[0;0m INFO 09-28 17:30:32 [core.py:218] init engine (profile, create kv cache, warmup model) took 20.74 seconds\n",
      "INFO 09-28 17:30:34 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 09-28 17:30:34 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "ckpt is ready.\n",
      "INFO 09-28 17:30:40 [chat_utils.py:538] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d6e76dd32a4735986bdbb5feaabad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/7405 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a1360a7f30482286ef2d7e50cdcc0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/7405 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain records: 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125d2be8f2a34894bd010af94106f292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3601ff3285a34998bf3430fab48b1c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/38 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving records to /home/kangjh/Research/ParametricReasoning/RPRAG/records.jsonl\n",
      "End at 2025-09-28 17:39:15\n",
      "Elapsed time: 567.51 seconds\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/kangjh/Research/ParametricReasoning/RPRAG/benchmark/NaiveGeneration/conf/config.yaml'\n",
    "cfg = OmegaConf.load(file_path)\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "start = time.time()\n",
    "print(f\"Start at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start))}\")\n",
    "\n",
    "solve(cfg)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"End at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end))}\")\n",
    "elapsed_time = end - start\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86d796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset = load_dataset('hotpotqa/hotpot_qa', 'fullwiki')['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036b5f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context'],\n",
       "    num_rows: 7405\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54494afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3bee1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chief of Protocol'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc75139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
