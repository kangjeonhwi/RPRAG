{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc809ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-30 23:40:10 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json, os, random\n",
    "import pdb\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import faiss\n",
    "\n",
    "import hydra \n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from FlagEmbedding import FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6029ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import json, os, random\n",
    "import pdb\n",
    "from datasets import load_dataset\n",
    "import tqdm\n",
    "import requests\n",
    "import time\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "def mystrip(one_str):\n",
    "    one_str = one_str.strip()\n",
    "    one_str = one_str.strip(\"\\\\n\")\n",
    "    one_str = one_str.strip(\"#\")\n",
    "    return one_str\n",
    "\n",
    "def extract_substring2(text, start_str, stop_strs):\n",
    "    start_index = text.find(start_str)\n",
    "    if start_index == -1:\n",
    "        return None\n",
    "    start = start_index + len(start_str)\n",
    "    \n",
    "    end = len(text)\n",
    "    \n",
    "    for stop_str in stop_strs:\n",
    "        temp_index = text.find(stop_str, start)\n",
    "        if temp_index != -1 and temp_index < end:\n",
    "            end = temp_index\n",
    "    if start < end:\n",
    "        return mystrip(text[start:end])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def split_response(response):\n",
    "    mydict = {\n",
    "        \"original\":response\n",
    "    }\n",
    "    str_analysis = \"The problem analysis:\"\n",
    "    str_query = \"The retrieval query:\"\n",
    "    str_answer = \"The final answer:\"\n",
    "    stop_strs = [str_analysis, str_query, str_answer, \"The retrieval documents:\", \"###\", \"####\"]\n",
    "    stop_strs_query = [str_analysis, str_query, str_answer, \"The retrieval documents:\", \"###\", \"####\", \"\\nStep\", \"?\"]\n",
    "    stop_strs_answer = [str_analysis, str_query, str_answer, \"The retrieval documents:\", \"###\", \"####\", \"\\nStep\"]\n",
    "    \n",
    "    start_index = response.find(str_analysis)\n",
    "    if start_index==-1:    \n",
    "        mydict['analysis']=None\n",
    "        return mydict\n",
    "    else:\n",
    "        mydict[\"analysis\"]=extract_substring2(response, str_analysis, stop_strs)\n",
    "    start_index_query = response.find(str_query, start_index+len(str_analysis))\n",
    "    start_index_answer = response.find(str_answer, start_index+len(str_analysis))\n",
    "    if start_index_query==-1 and start_index_answer==-1:\n",
    "        mydict['analysis']=None\n",
    "        return mydict\n",
    "    elif start_index_query!=-1 and start_index_answer!=-1:\n",
    "        if start_index_query<start_index_answer:\n",
    "            mydict['query']=extract_substring2(response[start_index_query:], str_query, stop_strs_query)\n",
    "        else:\n",
    "            mydict['answer']=extract_substring2(response[start_index_answer:], str_answer, stop_strs_answer)\n",
    "    elif start_index_query!=-1:\n",
    "        mydict['query']=extract_substring2(response[start_index_query:], str_query, stop_strs_query)\n",
    "    elif start_index_answer!=-1:\n",
    "        mydict['answer']=extract_substring2(response[start_index_answer:], str_answer, stop_strs_answer)\n",
    "    else:\n",
    "        raise ValueError\n",
    "    return mydict\n",
    "\n",
    "def GetRetrieval(retrieve_url: str, querys: list, cfg: DictConfig):\n",
    "    res = []\n",
    "    for i in tqdm.tqdm(range(0, len(querys), cfg.retrieval.post_batch_size), desc=\"Retrieving documents\"):\n",
    "        subset = querys[i:i + cfg.retrieval.post_batch_size]\n",
    "        for _ in range(cfg.retrieval.ssl_retry):\n",
    "            try:\n",
    "                response = requests.post(retrieve_url, json={\"queries\": subset}, headers={\"Content-Type\": \"application/json\"})\n",
    "                if response.status_code == 200 and response.json():\n",
    "                    res.extend(response.json())\n",
    "                    break\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Request failed: {e}, retrying...\")\n",
    "                time.sleep(2) # 재시도 전 잠시 대기\n",
    "        else:\n",
    "            # 최종적으로 실패한 경우\n",
    "            print(f\"Fail info: {response.text if 'response' in locals() else 'No response'}\")\n",
    "            raise ValueError(f\"Failed to retrieve query:{i} ~ {i + cfg.retrieval.post_batch_size}!!!!!!!!!!\")\n",
    "    return res\n",
    "\n",
    "def solve(cfg: DictConfig):\n",
    "    ckpt, records = solve_init(cfg)\n",
    "    solve_main(cfg, ckpt, records)\n",
    "    \n",
    "    remain_idxs = [i for i, record in enumerate(records) if 'answer' not in record]\n",
    "    print(f\"Remain records: {len(remain_idxs)}\")\n",
    "    \n",
    "    if len(remain_idxs) > 0:\n",
    "        solve_directly(cfg, ckpt, records)\n",
    "\n",
    "    with open(\"records.jsonl\", \"w\", encoding='utf-8') as f:\n",
    "        for record in records:\n",
    "            json.dump(record, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def solve_init(cfg: DictConfig):\n",
    "    llm_args = {\n",
    "        'model': cfg.model.path,\n",
    "        'tensor_parallel_size': cfg.model.tensor_parallel_size\n",
    "    }\n",
    "    if cfg.debug:\n",
    "        llm_args['tensor_parallel_size'] = 1\n",
    "\n",
    "    ckpt = LLM(**llm_args)\n",
    "    print(\"CKPT is ready.\")\n",
    "\n",
    "    dataset = dataset = load_dataset('hotpotqa/hotpot_qa', 'fullwiki')['validation']\n",
    "    \n",
    "    if cfg.debug:\n",
    "        dataset_size = len(dataset)\n",
    "        sample_size = min(8, dataset_size)\n",
    "        sampled_indices = random.sample(range(dataset_size), sample_size)\n",
    "        dataset = dataset.select(sampled_indices)\n",
    "\n",
    "    records = []\n",
    "    query_list = [data['question'] for data in dataset]\n",
    "    \n",
    "    for i, data in enumerate(dataset):\n",
    "        record = {\n",
    "            'question': data['question'],\n",
    "            'golden_answers': data['answer'],\n",
    "            'state': \"undo\",\n",
    "            'resample_times': 0\n",
    "        }\n",
    "        records.append(record)\n",
    "        \n",
    "    doc_list = GetRetrieval(cfg.retrieval.url, query_list, cfg)\n",
    "    \n",
    "    for doc_one, record in zip(doc_list, records):\n",
    "        record['doc'] = \"\\n\".join([doc_one_one['contents'] for doc_one_one in doc_one[:cfg.retrieval.num_of_docs]])\n",
    "        \n",
    "    return ckpt, records\n",
    "\n",
    "def generate_naive_rag_prompt(question, doc):\n",
    "    system_message = f\"\"\"Answer the question based on the given document. Only give me the answer and do not output any other words.\n",
    "The following are given documents.\n",
    "{doc}\n",
    "\"\"\"\n",
    "    user_message = f\"\"\"The question: {question}\"\"\"\n",
    "    message_list = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    return message_list\n",
    "def generate_naive_rag_cot_prompt(question, doc):\n",
    "    system_message = \"\"\"You are a helpful assistant that answers questions based on document retrieval with step-by-step reasoning.\n",
    "\n",
    "For any question, please structure your response in this format:\n",
    "The problem analysis: [Provide detailed step-by-step reasoning]\n",
    "The final answer: [Provide the concise final answer]\n",
    "\n",
    "Example:\n",
    "User: The question: What was the company's revenue in 2023?\n",
    "Assistant:\n",
    "The problem analysis: I need to find information about the company's revenue in 2023. Looking at the provided document, I can see in the third paragraph that \"the company's total revenue for fiscal year 2023 reached $128 million.\" This clearly states the exact revenue figure I'm looking for.\n",
    "The final answer: The company's revenue in 2023 was $128 million.\n",
    "\n",
    "Please carefully analyze the provided documents, ensure your answer is fully based on the document content, and use step-by-step reasoning to reach accurate conclusions.\"\"\"\n",
    "\n",
    "    system_message += f\"\"\"\n",
    "\n",
    "The following are the provided documents:\n",
    "{doc}\n",
    "\"\"\"\n",
    "\n",
    "    user_message = f\"\"\"The question: {question}\"\"\"\n",
    "    message_list = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    return message_list\n",
    "\n",
    "\n",
    "def solve_main(cfg: DictConfig, ckpt: LLM, records: list):\n",
    "    sampling_params = SamplingParams(temperature=cfg.params.temperature_main, max_tokens=cfg.params.max_tokens)\n",
    "    messages = [generate_naive_rag_cot_prompt(record['question'], record['doc']) for record in records]\n",
    "    \n",
    "    outputs = ckpt.chat(messages, sampling_params)\n",
    "    outputs = [output.outputs[0].text for output in outputs]\n",
    "    vals = [split_response(output) for output in outputs]\n",
    "        \n",
    "    for i, val in enumerate(vals):\n",
    "        records[i]['output'] = val['original']\n",
    "        if val.get('answer'):\n",
    "            records[i]['answer'] = val['answer']\n",
    "            records[i]['state'] = \"done\"\n",
    "        else:\n",
    "            records[i]['state'] = \"wrong\"\n",
    "\n",
    "def solve_directly(cfg: DictConfig, ckpt: LLM, records: list):\n",
    "    sampling_params = SamplingParams(temperature=cfg.params.temperature_main, max_tokens=cfg.params.max_tokens)\n",
    "    \n",
    "    remain_idxs = [i for i, record in enumerate(records) if 'answer' not in record]\n",
    "    messages = [generate_naive_rag_prompt(records[remain_idx]['question'], records[remain_idx]['doc']) for remain_idx in remain_idxs]\n",
    "    \n",
    "    outputs = ckpt.chat(messages, sampling_params)\n",
    "    outputs = [output.outputs[0].text for output in outputs]\n",
    "        \n",
    "    for output, remain_idx in zip(outputs, remain_idxs):  \n",
    "        records[remain_idx]['answer'] = output\n",
    "        records[remain_idx]['state'] = \"done\"\n",
    "        records[remain_idx]['resample_times'] = records[remain_idx].get('resample_times', 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b38e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88926168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaults:\n",
      "- _self_\n",
      "debug: false\n",
      "paths:\n",
      "  dev_dataset: /mnt/raid5/kangjh/downloads/datasets/hotpotqa/dev/dev.json\n",
      "  log_dir: ./logs\n",
      "model:\n",
      "  path: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "  tensor_parallel_size: 1\n",
      "retrieval:\n",
      "  url: http://10.0.12.120:8001/search_batch\n",
      "  post_batch_size: 2048\n",
      "  ssl_retry: 8\n",
      "  num_of_docs: 10\n",
      "params:\n",
      "  max_tokens: 512\n",
      "  temperature_main: 0.0\n",
      "  temperature_fallback: 0.0\n",
      "\n",
      "Start at 2025-09-30 23:40:12\n",
      "INFO 09-30 23:40:12 [utils.py:328] non-default args: {'disable_log_stats': True, 'model': 'meta-llama/Meta-Llama-3-8B-Instruct'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-30 23:40:21 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-30 23:40:22 [__init__.py:1815] Using max model len 8192\n",
      "INFO 09-30 23:40:25 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:26 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:26 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:29 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m WARNING 09-30 23:40:29 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:29 [gpu_model_runner.py:2338] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W930 23:40:29.042626747 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:30 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:30 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:30 [weight_utils.py:348] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb777102e9054e0ca068d73bdd1ad756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:35 [default_loader.py:268] Loading weights took 3.89 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:35 [gpu_model_runner.py:2392] Model loading took 14.9596 GiB and 5.016109 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:41 [backends.py:539] Using cache directory: /home/kangjh/.cache/vllm/torch_compile_cache/ec52f37144/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:41 [backends.py:550] Dynamo bytecode transform time: 5.35 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:43 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.839 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:44 [monitor.py:34] torch.compile takes 5.35 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:46 [gpu_worker.py:298] Available KV cache memory: 26.48 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:46 [kv_cache_utils.py:864] GPU KV cache size: 216,880 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:46 [kv_cache_utils.py:868] Maximum concurrency for 8,192 tokens per request: 26.47x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:51 [gpu_model_runner.py:3118] Graph capturing finished in 5 secs, took 0.53 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:51 [gpu_worker.py:391] Free memory on device (47.13/47.43 GiB) on startup. Desired GPU memory utilization is (0.9, 42.69 GiB). Actual usage is 14.96 GiB for weight, 1.24 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.53 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=27701056921` to fit into requested memory, or `--kv-cache-memory=32465419264` to fully utilize gpu memory. Current kv cache memory in use is 28428768665 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2338256)\u001b[0;0m INFO 09-30 23:40:51 [core.py:218] init engine (profile, create kv cache, warmup model) took 16.23 seconds\n",
      "INFO 09-30 23:40:53 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 09-30 23:40:53 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "CKPT is ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents:  25%|██▌       | 1/4 [49:04<2:27:14, 2944.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents:  25%|██▌       | 1/4 [1:18:23<3:55:09, 4703.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 10-01 00:59:22 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m start = time.time()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStart at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39mtime.localtime(start))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m end = time.time()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnd at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39mtime.localtime(end))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36msolve\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msolve\u001b[39m(cfg: DictConfig):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     ckpt, records = \u001b[43msolve_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     solve_main(cfg, ckpt, records)\n\u001b[32m     92\u001b[39m     remain_idxs = [i \u001b[38;5;28;01mfor\u001b[39;00m i, record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(records) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m record]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36msolve_init\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    126\u001b[39m     record = {\n\u001b[32m    127\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m: data[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    128\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgolden_answers\u001b[39m\u001b[33m'\u001b[39m: data[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    129\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mundo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mresample_times\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m    131\u001b[39m     }\n\u001b[32m    132\u001b[39m     records.append(record)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m doc_list = \u001b[43mGetRetrieval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieval\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc_one, record \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(doc_list, records):\n\u001b[32m    137\u001b[39m     record[\u001b[33m'\u001b[39m\u001b[33mdoc\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([doc_one_one[\u001b[33m'\u001b[39m\u001b[33mcontents\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc_one_one \u001b[38;5;129;01min\u001b[39;00m doc_one[:cfg.retrieval.num_of_docs]])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mGetRetrieval\u001b[39m\u001b[34m(retrieve_url, querys, cfg)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg.retrieval.ssl_retry):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m         response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretrieve_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqueries\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m response.json():\n\u001b[32m     77\u001b[39m             res.extend(response.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Research/ParametricReasoning/RPRAG/.venv/lib/python3.11/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/http/client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/http/client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/http/client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "file_path = '/home/kangjh/Research/ParametricReasoning/RPRAG/benchmark/NaiveRAG/conf/config.yaml'\n",
    "cfg = OmegaConf.load(file_path)\n",
    "print(OmegaConf.to_yaml(cfg))\n",
    "start = time.time()\n",
    "print(f\"Start at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start))}\")\n",
    "\n",
    "solve(cfg)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"End at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end))}\")\n",
    "elapsed_time = end - start\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents: 100%|██████████| 1/1 [00:14<00:00, 14.35s/it]\n"
     ]
    }
   ],
   "source": [
    "ret_result = GetRetrieval('http://10.0.12.120:8001/search_batch', ['What is the capital of France?', 'Who is the president of the United States?'], cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
