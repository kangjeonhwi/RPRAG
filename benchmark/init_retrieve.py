import json
import os
import time
import random
from typing import Dict, List, Any

import hydra
import tqdm
from omegaconf import DictConfig, OmegaConf
from flashrag.retriever import DenseRetriever

def run_preprocessing(cfg: DictConfig):
    """
    RAGë¥¼ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸.
    1. DenseRetrieverë¥¼ ë¡œì»¬ì—ì„œ ì§ì ‘ ì´ˆê¸°í™”í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ì˜¤ë²„í—¤ë“œ ì œê±°.
    2. GPUë¥¼ ì‚¬ìš©í•œ FAISS ì¸ë±ì‹±ìœ¼ë¡œ ê²€ìƒ‰ ì†ë„ ê°€ì†í™”.
    3. ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ì¦‰ì‹œ íŒŒì¼ì— ì €ì¥í•˜ì—¬, ì¤‘ë‹¨ ì‹œ í•´ë‹¹ ì§€ì ë¶€í„° ì´ì–´í•˜ê¸° ê¸°ëŠ¥ êµ¬í˜„.
    """
    input_file = cfg.input_file
    output_file = cfg.rag_params.preprocessed_file

    print("--- ğŸš€ Starting RAG Data Pre-processing ---")

    processed_questions = set()
    if os.path.exists(output_file):
        print(f"âœ… Output file '{output_file}' found. Checking for completed records to resume.")
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    # ê° ë¼ì¸ì´ ìœ íš¨í•œ JSONì¸ì§€ í™•ì¸
                    if line.strip():
                        record = json.loads(line)
                        # 'problem' ë˜ëŠ” 'question' í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ  ì§ˆë¬¸ ì‹ë³„
                        question = record.get('problem', record.get('question'))
                        if question:
                            processed_questions.add(question)
            print(f"ğŸ“ˆ Found {len(processed_questions)} already processed records. Will skip them.")
        except (json.JSONDecodeError, IOError) as e:
            print(f"ğŸ¤” WARNING: Could not parse output file '{output_file}'. Starting from scratch. Error: {e}")
            # ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ì€ ë®ì–´ì“°ê¸° ìœ„í•´ processed_questionsë¥¼ ë¹„ì›€
            processed_questions.clear()


    # --- ë°ì´í„° ë¡œë”© ë° ì²˜ë¦¬í•  ë°ì´í„° ì„ ë³„ (2/3) ---
    print(f"Loading records from: {input_file}")
    all_records = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            all_records.append(json.loads(line))
            
    # ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë ˆì½”ë“œë§Œ í•„í„°ë§
    records_to_process = [
        rec for rec in all_records
        if rec.get('problem', rec.get('question', '')) not in processed_questions
    ]
    
    total_count = len(all_records)
    processed_count = len(processed_questions)
    to_process_count = len(records_to_process)

    print(f"ğŸ“Š Total records: {total_count} | Already processed: {processed_count} | New records to process: {to_process_count}")

    if not records_to_process:
        print("ğŸ‰ No new records to process. Pre-processing is already complete.")
        return

    # ë””ë²„ê·¸ ëª¨ë“œì¼ ê²½ìš°, ì²˜ë¦¬í•  ë ˆì½”ë“œ ì¤‘ì—ì„œ ìƒ˜í”Œë§
    if cfg.debug:
        sample_size = min(8, len(records_to_process))
        records_to_process = random.sample(records_to_process, sample_size)
        print(f"ğŸ Debug mode: Sampled {len(records_to_process)} new records for pre-processing.")

    print("\nInitializing DenseRetriever... (This may take a moment)")
    retriever_config = OmegaConf.to_container(cfg.retriever, resolve=True)
    dense_retriever = DenseRetriever(retriever_config)
    print("âœ… Retriever initialized successfully.")

    all_questions = [rec.get('problem', rec.get('question', '')) for rec in records_to_process]
    batch_size = retriever_config.get('retrieval_batch_size', 32)


    with open(output_file, "a", encoding='utf-8') as f:
        # tqdmì„ ì‚¬ìš©í•˜ì—¬ ë°°ì¹˜ ì²˜ë¦¬ ì§„í–‰ ìƒí™© ì‹œê°í™”
        for i in tqdm.tqdm(range(0, len(all_questions), batch_size), desc="Retrieving documents"):
            batch_questions = all_questions[i:i + batch_size]
            batch_records = records_to_process[i:i + batch_size]
            
            try:
                # retrieverì˜ batch_search ì§ì ‘ í˜¸ì¶œ
                retrieved_docs_batch = dense_retriever.batch_search(batch_questions)

                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ê²°ê³¼ ì²˜ë¦¬ ë° íŒŒì¼ ì €ì¥
                for rec, docs in zip(batch_records, retrieved_docs_batch):
                    doc_contents = [d.get('contents', '') for d in docs[:cfg.rag_params.num_of_docs]]
                    rec['doc'] = "\n".join(doc_contents)
                    
                    # JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ íŒŒì¼ì— í•œ ì¤„ì”© ì“°ê¸° (ì¦‰ì‹œ ì €ì¥)
                    json.dump(rec, f, ensure_ascii=False)
                    f.write('\n')

            except Exception as e:
                print(f"\nâŒ CRITICAL ERROR during retrieval batch: {e}")
                print("ğŸ›‘ Aborting pre-processing. You can restart the script to resume from the last saved point.")
                return

    print(f"\nğŸ’¾ RAG pre-processing complete. {len(records_to_process)} new records saved to: {output_file}")


@hydra.main(version_base=None, config_path="conf", config_name="retrieve_config")
def main(cfg: DictConfig):
    run_preprocessing(cfg)

if __name__ == "__main__":
    main()