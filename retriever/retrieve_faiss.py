import json
import os
import faiss
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any


class Settings:
    # ê²½ë¡œ ì„¤ì •
    retrieval_model_path: str = '/mnt/raid5/kangjh/downloads/e5-base-v2'
    index_path: str = "/mnt/raid5/kangjh/downloads/Tevatron/wikipedia-nq-corpus-flashragformat/index/e5_ivfpq.index"
    corpus_path: str = "/mnt/raid5/kangjh/downloads/Tevatron/wikipedia-nq-corpus-flashragformat/processed_corpus.jsonl"

    # Retriever ì„±ëŠ¥ íŒŒë¼ë¯¸í„°
    retrieval_topk: int = 5
    retrieval_batch_size: int = 128  # API ë°°ì¹˜ ê²€ìƒ‰ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°
    retrieval_use_fp16: bool = True
    retrieval_nprobe: int = 2048

# --------------------------------------------------------------------------
# 2. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ë° Pydantic ëª¨ë¸ ì •ì˜
# --------------------------------------------------------------------------
class QueryRequest(BaseModel):
    query: str

class BatchQueryRequest(BaseModel):
    queries: List[str]

# ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
settings = Settings()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="GPU-Accelerated Faiss Retriever Service",
    version="2.0",
    description="A high-performance retriever using SentenceTransformers and faiss-gpu."
)

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸, ì½”í¼ìŠ¤, GPU ì¸ë±ìŠ¤ë¥¼ ê´€ë¦¬
# ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œë©ë‹ˆë‹¤.
model: SentenceTransformer = None
corpus: List[str] = None
gpu_index: faiss.GpuIndex = None

# --------------------------------------------------------------------------
# 3. ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë”©
# --------------------------------------------------------------------------
@app.on_event("startup")
def load_retriever_components():
    """
    FastAPI ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ëª¨ë¸, ì½”í¼ìŠ¤, Faiss ì¸ë±ìŠ¤ë¥¼ GPUì— ë¡œë“œí•©ë‹ˆë‹¤.
    """
    global model, corpus, gpu_index

    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is not available. This server requires a GPU.")

    device = 'cuda'
    print(f"--- ğŸš€ Loading Retriever Components to {device.upper()} ---")

    # 1. SentenceTransformer ëª¨ë¸ ë¡œë”©
    print(f"Loading embedding model from: {settings.retrieval_model_path}")
    model = SentenceTransformer(settings.retrieval_model_path, device=device)
    if settings.retrieval_use_fp16:
        model.half()
    print("âœ… Embedding model loaded.")

    # 2. ì½”í¼ìŠ¤ ë¡œë”©
    print(f"Loading corpus from: {settings.corpus_path}")
    try:
        corpus = []
        with open(settings.corpus_path, 'r', encoding='utf-8') as f:
            for line in f:
                corpus.append(json.loads(line).get('contents', ''))
        print(f"âœ… Corpus loaded with {len(corpus)} documents.")
    except Exception as e:
        raise RuntimeError(f"Failed to load corpus file: {e}")

    # 3. Faiss ì¸ë±ìŠ¤ ë¡œë”© ë° GPUë¡œ ì´ë™
    print(f"Loading FAISS index from: {settings.index_path}")
    try:
        cpu_index = faiss.read_index(settings.index_path)
        cpu_index.nprobe = settings.retrieval_nprobe
        print(f"âœ… Index loaded on CPU. nprobe set to {cpu_index.nprobe}.")
        
        print("Moving FAISS index to GPU...")
        res = faiss.StandardGpuResources()
        co = faiss.GpuClonerOptions()
        co.useFloat16 = settings.retrieval_use_fp16
        gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)
        print("âœ… FAISS index is now on GPU and ready to serve.")
    except Exception as e:
        raise RuntimeError(f"Failed to load or move FAISS index to GPU: {e}")

# --------------------------------------------------------------------------
# 4. ê²€ìƒ‰ ë¡œì§ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
# --------------------------------------------------------------------------
def _perform_search(queries: List[str]) -> List[List[Dict[str, Any]]]:
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ GPU Faiss ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        prefixed_queries = [f"query: {q}" for q in queries]

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embeddings = model.encode(
            prefixed_queries,
            batch_size=settings.retrieval_batch_size,
            normalize_embeddings=True,
            show_progress_bar=False,
            convert_to_numpy=True
        )

        # Faiss ê²€ìƒ‰
        distances, ids = gpu_index.search(query_embeddings.astype(np.float32), settings.retrieval_topk)

        # ê²°ê³¼ í¬ë§·íŒ…
        all_results = []
        for i, query in enumerate(queries):
            query_results = []
            for j in range(settings.retrieval_topk):
                doc_id = ids[i][j]
                if doc_id != -1:
                    query_results.append({
                        "id": int(doc_id),
                        "content": corpus[doc_id],
                        "score": float(distances[i][j])
                    })
            all_results.append(query_results)
        
        return all_results

    except Exception as e:
        print(f"Error during search: {e}")
        # ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ë” ìƒì„¸í•œ ë¡œê¹…ì´ í•„ìš”í•©ë‹ˆë‹¤.
        raise HTTPException(status_code=500, detail=f"An internal error occurred during search: {e}")


@app.post("/search", response_model=List[Dict[str, Any]])
def search(request: QueryRequest):
    """
    ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not request.query:
        raise HTTPException(status_code=400, detail="Query cannot be empty.")
    
    results = _perform_search([request.query])
    return results[0]

@app.post("/search_batch", response_model=List[List[Dict[str, Any]]])
def search_batch(request: BatchQueryRequest):
    """
    ì—¬ëŸ¬ ì¿¼ë¦¬ì— ëŒ€í•œ ë°°ì¹˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not request.queries:
        raise HTTPException(status_code=400, detail="Queries cannot be empty.")
    
    return _perform_search(request.queries)

if __name__ == "__main__":
    # uvicorn your_script_name:app --host 0.0.0.0 --port 8000
    # ì˜ˆ: uvicorn main:app --host 0.0.0.0 --port 8000
    uvicorn.run(app, host="0.0.0.0", port=8001)